{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpvr3PuOBDCN+sDMLtBeL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzh2027/zzh2027.github.io/blob/master/Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Pa5pLsJ-TC",
        "colab_type": "text"
      },
      "source": [
        "##1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnTLpivWJhiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx4_e3X0Jj4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.signal import argrelextrema\n",
        "from collections import defaultdict\n",
        "import pandas_datareader.data as web\n",
        "import ta\n",
        "from scipy import interpolate\n",
        "from scipy.interpolate import UnivariateSpline\n",
        "\n",
        "import sys\n",
        "sys.setrecursionlimit(10**6) \n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhMQXRz4J87U",
        "colab_type": "text"
      },
      "source": [
        "##2. The Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo6dmfP3JlVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PipeLine(object):\n",
        "    def __init__(self, df):\n",
        "        \"\"\" \n",
        "        features --> Open, High, Low, Close, Volume, news, P/E ratio,\n",
        "        \"\"\"\n",
        "        if df:\n",
        "            self.df = df\n",
        "            self.original_df = df.copy()\n",
        "            print(\"Begin to process {} stocks...\".format(str(len(self.df))))\n",
        "            print(self.df)\n",
        "        else:\n",
        "            self.df, self.original_df = None, None\n",
        "        self.model_df = None\n",
        "\n",
        "    def GetQuandlData(self, symbols, begin_date = None, end_date = None, api_key = \"fsRzwWgnx-zG8H2L9ya2\"):\n",
        "        \"\"\" test data\"\"\"\n",
        "        os.environ['QUANDL_API_KEY']  = api_key\n",
        "        out = pd.DataFrame()\n",
        "        data_source = 'quandl'\n",
        "        for symbol in symbols:\n",
        "            df = web.DataReader(symbol, data_source, begin_date, end_date)[['AdjOpen', 'AdjHigh', 'AdjLow', 'AdjClose', 'AdjVolume']].reset_index()\n",
        "            df.columns = ['date','open','high','low','close','volume']\n",
        "            df['symbol'] = symbol\n",
        "            df = df.set_index(['date', 'symbol'])\n",
        "            out = pd.concat([out,df], axis = 0)\n",
        "        self.original_df = out.sort_index()\n",
        "        self.df = self.original_df.copy()\n",
        "        print(self.df.tail())\n",
        "        return self.original_df\n",
        "\n",
        "    def GetTiingoData(self, symbol, begin_date = None, end_date = None, api_key = \"ab583c49e5bed14e7c1b32145f44c60961199310\"):\n",
        "        try:\n",
        "            df = pdr.get_data_tiingo(symbol, api_key = api_key)[['adjOpen', 'adjHigh', 'adjLow', 'adjClose', 'volume']].reset_index()\n",
        "            df.columns = ['symbol','date','open','high','low','close','volume']\n",
        "            df = df.set_index(['date', 'symbol'])\n",
        "        except TypeError:\n",
        "            print('Stock {} is not found!'.format(symbol))\n",
        "        self.original_df = df.sort_index()\n",
        "        self.df = self.original_df.copy()\n",
        "        print(self.df.tail())\n",
        "        return self.original_df\n",
        "        \n",
        "\n",
        "\n",
        "    def Reboot(self):\n",
        "        self.df = self.original_df.copy()\n",
        "\n",
        "    def DigestNews(self):\n",
        "        \"\"\" Generate a NLP object and convert NEWS into categorical or numeric variables \"\"\"\n",
        "        tmp = NLP(self.df)\n",
        "        \"\"\" This class should be able to return a dataframe using a method named 'helper' for now \"\"\"\n",
        "        return tmp.helper(self.df)\n",
        "    \n",
        "    def CustomMetrics(self):\n",
        "        \"\"\" create some simple metrics \"\"\"\n",
        "        self._pctChange()\n",
        "        self._MACD()\n",
        "        self._featureEngineering()\n",
        "\n",
        "        \n",
        "        ## MA\n",
        "        #for n in [14, 30, 50, 200]:\n",
        "            #self.df.loc['ma' + str(n)] = talib.SMA\n",
        "        ## RSI\n",
        "        print(self.df)\n",
        "    def _featureEngineering(self, n_bins = 10):\n",
        "        df = self.df.copy()\n",
        "        self.df['f01'] = df.close/df.open - 1\n",
        "        self.df['f02'] = df.open / df.close.shift(1) - 1\n",
        "        self.df['f03'] = df.volume.apply(np.log)\n",
        "        self.df['f04'] = df.volume.diff()\n",
        "        self.df['f05'] = df.volume.pct_change()\n",
        "        self.df['f06'] = df.groupby(level = 'symbol').volume.apply(lambda x:x.rolling(5).mean()).apply(np.log)\n",
        "        if len(self.df) > 2000:\n",
        "            self.df['f07'] = df.volume.diff(50)\n",
        "            self.df['f08'] = df.volume/df.groupby(level = 'symbol').volume.apply(lambda x:x.rolling(200).mean()) - 1\n",
        "            self.df['f09'] = df.close/df.groupby(level = 'symbol').close.apply(lambda x:x.ewm(50).mean()) - 1\n",
        "            zscore_func = lambda x:(x-x.rolling(window = 200, min_periods = 20).mean()) /x.rolling(window = 200, min_periods = 20).std()\n",
        "            self.df['f10'] = df.groupby(level = 'symbol').close.apply(zscore_func)##z-score\n",
        "            percentile_func = lambda x:x.rolling(200, min_periods = 20).apply(lambda x:pd.Series(x).rank(pct = True)[0])\n",
        "        else:\n",
        "            self.df['f07'] = df.volume.diff(20)\n",
        "            self.df['f08'] = df.volume/df.groupby(level = 'symbol').volume.apply(lambda x:x.rolling(28).mean()) - 1\n",
        "            self.df['f09'] = df.close/df.groupby(level = 'symbol').close.apply(lambda x:x.ewm(28).mean()) - 1\n",
        "            zscore_func = lambda x:(x-x.rolling(window = 200, min_periods = 14).mean()) /x.rolling(window = 200, min_periods = 14).std()\n",
        "            self.df['f10'] = df.groupby(level = 'symbol').close.apply(zscore_func)##z-score\n",
        "            percentile_func = lambda x:x.rolling(200, min_periods = 14).apply(lambda x:pd.Series(x).rank(pct = True)[0])\n",
        "        self.df['f11'] = df.groupby(level = 'symbol').volume.apply(percentile_func)\n",
        "        self.df['f12'] = self.df['f08'].dropna().rank(pct= True)##rank each stock cross-sectionally \n",
        "\n",
        "        from pyti.money_flow_index import money_flow_index as mfi\n",
        "        self.df['f13'] = mfi(df.high, df.low, df.close, df.volume, period = 14)##Money flow index(14 days)\n",
        "        self.df['f14'] = self.df['f13'] - self.df['f13'].rolling(200, min_periods = 10).mean()##mean-centered money flow index\n",
        "        \n",
        "        binning_func =  lambda y:pd.qcut(y, q = n_bins, labels = range(1, n_bins+1))\n",
        "        self.df['f15'] = df.volume.groupby(level = 'symbol').apply(binning_func)\n",
        "        self.df['f16'] = self.df['f06'].apply(np.sign)\n",
        "        plus_minus_fxn = lambda x:x.rolling(20).sum()\n",
        "        self.df['f17'] = self.df['f16'].groupby(level = 'symbol').apply(plus_minus_fxn)\n",
        "\n",
        "    def _pctChange(self):\n",
        "        \"\"\"Change of the open price of tomorrow compared to the close price today.\"\"\"\n",
        "        self.df['close_1'] = self.df.close.pct_change(-1)\n",
        "        self.df['close_5'] = self.df.close.pct_change(-5)\n",
        "        self.df['close_10'] = self.df.close.pct_change(-10)\n",
        "        self.df['close_20'] = self.df.close.pct_change(-20)\n",
        "\n",
        "    def _MACD_helper(self, close):\n",
        "        exp1 = close.ewm(span = 12, adjust = False).mean()\n",
        "        exp2 = close.ewm(span = 26, adjust = False).mean()\n",
        "        DIF = exp1 - exp2\n",
        "        DEA = DIF.ewm(span = 9, adjust = False).mean()\n",
        "        return DIF, DEA\n",
        "        \n",
        "    def _MACD_trend(self, row):\n",
        "        # method 1 --> walk-forward for 5 days each and interpolate()\n",
        "        # method 2 --> \n",
        "        day_diff = lambda x: (x-self.df.date[0]).days\n",
        "        self.df['day'] = self.df.date.apply(day_diff)\n",
        "        X0, y0 = self.df.iloc[row-4:row+1]['day'].values.reshape(-1,1), self.df.iloc[row-4:row+1]['MACD_bar'].values.reshape(-1,1)\n",
        "        model = LinearRegression().fit(X0, y0)\n",
        "        res = model.coef_[0][0]\n",
        "        #print(\"最后一行的值是{}\".format(res))\n",
        "        self.df.slope_MACD_bar.iloc[row] = res\n",
        "        row-=1\n",
        "        while row >= 0:\n",
        "            for i in range(4,-1,-1):\n",
        "                if row - i >=0:\n",
        "                    y = self.df.iloc[row-i:row+1]['MACD_bar'].values.reshape(-1,1)## row ->9  -->第八行  row-->1   -->第零行\n",
        "                    X = self.df.iloc[row-i:row+1]['day'].values.reshape(-1,1)\n",
        "                    model = LinearRegression().fit(X,y)\n",
        "                    self.df.loc[row,\"slope_MACD_bar\"] = (4/5 * model.coef_[0][0]) + (1/5 * round(self.df.loc[row+1,\"slope_MACD_bar\"],4))\n",
        "                    break\n",
        "            row-=1\n",
        "\n",
        "            \n",
        "    def _MACD(self):\n",
        "        \"\"\"\n",
        "        Bullish crossover, just like in Moving Averages, a buy signal occurs when MACD crosses above the signal line. \n",
        "        A bearish signal occurs when MACD crosses below the signal line. \n",
        "        \"\"\"\n",
        "        DIF, DEA= self._MACD_helper(self.df.close)\n",
        "        self.df['MACD'] = DIF\n",
        "        self.df['Signal'] = DEA\n",
        "        self.df['MACD_bar'] = (DIF - DEA*2)\n",
        "        \"\"\"\n",
        "        Since the MACD's trend is of matters, we need to calculate its trend through LR first\n",
        "        The time period is set as 5 days for now\n",
        "        \"\"\"\n",
        "        self.df['slope_MACD_bar'] = np.nan\n",
        "        self.df = self.df.reset_index()\n",
        "        #print(self.df.columns)\n",
        "        self._MACD_trend(len(self.df) - 1)\n",
        "        self.df = self.df.set_index(['date', 'symbol'])\n",
        "\n",
        "    def _get_max_min(self, df, MA = 1, window_range = 4, greatest = False):\n",
        "        \"\"\" return max and min values and index \"\"\"\n",
        "        df_ma = df.close.rolling(window = MA).mean().dropna()## take the Moving Average value to analyze\n",
        "        local_max = argrelextrema(df_ma.values, np.greater)[0]## get the index with greater values\n",
        "        local_min = argrelextrema(df_ma.values, np.less)[0]   ## get the index with smaller values\n",
        "        if greatest:\n",
        "            return df.iloc[local_max], df.iloc[local_min]## For triangle trend analyze, we only need greater values\n",
        "\n",
        "        ## Select the spike and the valley    \n",
        "        price_local_max_dt = []\n",
        "        for i in local_max:\n",
        "            if (i > window_range) and (i < len(df_ma) - window_range):\n",
        "                price_local_max_dt.append(df.iloc[i-window_range: i+window_range]['close'].idxmax())## 得到一定范围内值最大的位置\n",
        "        price_local_min_dt = []\n",
        "        for i in local_min:\n",
        "            if (i > window_range) and (i < len(df_ma) - window_range):\n",
        "                price_local_min_dt.append(df.iloc[i-window_range: i+window_range]['close'].idxmin())## 得到一定范围内值最小的位置\n",
        "        maxima = pd.DataFrame(df.loc[price_local_max_dt])\n",
        "        minima = pd.DataFrame(df.loc[price_local_min_dt])\n",
        "        return maxima, minima\n",
        "\n",
        "    def TriangleTrend(self,  MA=1, window_range =4, greatest = True):\n",
        "        \"\"\" a Converging price range \"\"\" \n",
        "        #1 slice dataframe into multiple parts\n",
        "        sliced_date = self.df.resample('M', level = 'date').mean().index.values\n",
        "        out = pd.DataFrame()\n",
        "        for d in sliced_date:\n",
        "            X = self.df.xs(slice(d - pd.Timedelta('30 days'), d), level = 'date', drop_level = False )\n",
        "            higher, lower = self._get_max_min(X, MA, window_range, greatest)\n",
        "            spikes, downs = higher[['close', 'day']], lower[['close', 'day']]\n",
        "            #spikes.date = spikes.date.apply(lambda x: (x-spikes.date[0]).days)\n",
        "        ### Meethod without self.get_max_min() to get spikes and downs\n",
        "        #spikes = [[0,b[0]],[len(b)-1,b[-1]]]\n",
        "        #downs  = [[0,b[0]],[len(b)-1,b[-1]]]\n",
        "        #for i in range(1,len(b)-1):\n",
        "        #    if b[i] > b[i-1] and b[i]>b[i+1]:\n",
        "        #        spikes.append((i,b[i]))\n",
        "        #    elif b[i] < b[i-1] and b[i] < b[i+1]:\n",
        "        #        downs.append((i,b[i]))\"\"\"\n",
        "        #return spikes, downs\n",
        "        \n",
        "            res = []\n",
        "            for i in [spikes, downs]:\n",
        "                X,y = pd.DataFrame(i.day), pd.DataFrame(i.close)\n",
        "                model = LinearRegression().fit(X,y)\n",
        "                res.append(model.coef_[0][0])\n",
        "\n",
        "            ratio_t = abs((res[0] - res[1])/np.mean(res))\n",
        "            if ratio_t < 0.05:\n",
        "                X['TriangleTrend'] = ratio_t# symmetrical\n",
        "            else:\n",
        "                val = np.sqrt(abs(res[1]**2 - res[0]**2))\n",
        "                mid = np.mean(res)\n",
        "                res = val#np.log(val)*np.sign(mid)\n",
        "                X['TriangleTrend'] = res\n",
        "                \"\"\"\n",
        "                if res <0:\n",
        "                    print('It might keep descending after {}'.format(str(d)))\n",
        "                else:\n",
        "                    print(\"It is going up after {}\".format(str(d)))     \n",
        "                \"\"\"   \n",
        "            out = pd.concat([out, X])\n",
        "        self.df  = pd.merge(self.df, out, how = 'left', on = ['date', 'symbol']).drop(columns = 'day_y')\n",
        "        small = self.df.loc[~self.df.TriangleTrend.isnull()]\n",
        "        fill = UnivariateSpline(small['day_x'].values, small['TriangleTrend'].values, s=1)\n",
        "        xint = np.linspace(0, self.df['day_x'].values[-1] , len(self.df))\n",
        "        yint = fill(xint)\n",
        "        self.df.TriangleTrend = yint  \n",
        "        self.df = self.df.rename(columns = {'day_x':'day'})\n",
        "        return self.df\n",
        "\n",
        "    def HeadAndShoulders(self):\n",
        "        \"\"\" Predicts a bullish-to-bearish trend reversal \"\"\"\n",
        "        sliced_date = self.df.resample('3M', level = 'date').mean().index.values\n",
        "        out = pd.DataFrame()\n",
        "        for d in sliced_date:\n",
        "            X = self.df.xs(slice(d - pd.Timedelta('93 days'), d), level = 'date', drop_level = False )\n",
        "            patterns = self._findPatterns(df = X).fillna(method = 'ffill')\n",
        "            if patterns.__len__() == 0:\n",
        "                print('No patterns detected.')\n",
        "                continue\n",
        "            try:\n",
        "                bull, bear = patterns.bull.iloc[-1], patterns.bear.iloc[-1]## pick the latest pattern as the feature value\n",
        "                if bull[0] > bear[0]:\n",
        "                    X['HS'] = 1\n",
        "                else:\n",
        "                    X['HS'] = -1\n",
        "                \n",
        "            except AttributeError:\n",
        "                X['HS'] = 1 if patterns.columns[0] == 'bull' else -1# In case, there is only one pattern\n",
        "            out = pd.concat([out,X])\n",
        "        \n",
        "        self.df = out.copy()\n",
        "        return self.df\n",
        "        \n",
        "\n",
        "    \n",
        "    def _findPatterns(self, df):\n",
        "        a,b = self._get_max_min(df = df, MA = 3, window_range = 4)\n",
        "        #\n",
        "        max_min = pd.concat([a,b]).close.reset_index().set_index('date').sort_index()\n",
        "        patterns = defaultdict(list)\n",
        "        \n",
        "        for i in range(5, len(max_min)):\n",
        "            window = max_min.iloc[i-5: i]\n",
        "\n",
        "            if (window.index[-1] - window.index[0]).days >100:\n",
        "                print('buxing')\n",
        "                continue## If the days difference is greater than 100 days, it might be too much, then we pass this one\n",
        "            a,b,c,d,e= window.iloc[0].close,window.iloc[1].close,window.iloc[2].close,window.iloc[3].close,window.iloc[4].close\n",
        "\n",
        "            if a<b and c<a and c<e and c<d and e<d and abs(b-d)<=np.mean([b,d])*0.02:\n",
        "                patterns['IHS'].append((window.index[0], window.index[-1]))## this one is basically abandoned\n",
        "            if (c<a or c<b) and (c<e or c<d) and (abs(a-e)/np.mean([a,e]) <= 0.02 or abs(a-d)/np.mean([a,d]) <= 0.02 or abs(b-e)/np.mean([b,e]) <= 0.02):\n",
        "                #print('bull-->{}'.format([a,b,c,d,e]))\n",
        "                patterns['bull'].append((window.index[0], window.index[-1]))\n",
        "            if (c>a or c>b) and (c>e or c>d) and (abs(a-e)/np.mean([a,e]) <= 0.02 or abs(a-d)/np.mean([a,d]) <= 0.02 or abs(b-e)/np.mean([b,e]) <= 0.02):\n",
        "                #print('bear-->{}'.format([a,b,c,d,e]))\n",
        "                patterns['bear'].append((window.index[0], window.index[-1]))\n",
        "        res = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in patterns.items()]))\n",
        "        return res\n",
        "\n",
        "    def GetTargetVar(self, MA_window = 5):\n",
        "        self.df = self.df.set_index('day')\n",
        "        tarVarName = 'MA_' + str(MA_window) + '_close'\n",
        "        f_1 = pd.Series(self.df['close'].values[::-1])##pick the close price and reverse for MA calculation\n",
        "        f_2 = f_1.rolling(window = MA_window).mean()## calculate the future MA price\n",
        "        f_3 = pd.DataFrame(f_2[::-1]).reset_index(drop = False)\n",
        "        f_3.index = self.df.index\n",
        "        self.df = pd.concat([self.df, f_3], axis = 1).drop(columns = ['index']).rename(columns = {0:tarVarName})## insert the target variables back to self.df\n",
        "        self.df['LS_signal'] = self.df[tarVarName] - self.df.close\n",
        "        self.model_df  = self.df.loc[~self.df.LS_signal.isnull()]\n",
        "        self.model_df = self.model_df.dropna()\n",
        "        return self.model_df\n",
        "\n",
        "    def ModelSelection(self):\n",
        "        y = self.model_df[['LS_signal']].apply(np.sign)\n",
        "        features = self.model_df.columns[:-2]\n",
        "        X = self.model_df[features]\n",
        "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
        "        print('training data has %d observation with %d features'% X_train.shape)\n",
        "        print('test data has %d observation with %d features'% X_test.shape)\n",
        "        print(\"=================Default Model Performance===========================================\")\n",
        "        classifier_logistic = LogisticRegression()\n",
        "        classifier_KNN = KNeighborsClassifier()\n",
        "        classifier_RF = RandomForestClassifier()\n",
        "        model_names = ['Logistic Regression','KNN','Random Forest']\n",
        "        model_list = [classifier_logistic, classifier_KNN, classifier_RF]\n",
        "        res = []\n",
        "        for classifier in model_list:\n",
        "            cv_score = model_selection.cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "            res.append(cv_score.mean())\n",
        "        for i in range(len(model_names)):\n",
        "            print('Model accuracy of %s is: %.3f'%(model_names[i],res[i]))\n",
        "        print(\"====================Grid Search for Better Models==========================================\")\n",
        "        def print_grid_search_metrics(gs,parameters):\n",
        "            print (\"Best score: %0.3f\" % gs.best_score_)\n",
        "            print (\"Best parameters set:\")\n",
        "            best_parameters = gs.best_params_\n",
        "            for param_name in sorted(parameters.keys()):\n",
        "                print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))   \n",
        "            \n",
        "        parameters_lr  = {'penalty':('l1', 'l2', \"elasticnet\", \"None\"), \n",
        "                         'C':(1, 5, 10)}\n",
        "        parameters_knn = {'n_neighbors':[3,5,7,10]}\n",
        "        parameters_rf  = {'n_estimators' : [40,60,80]}\n",
        "        parameters_list = [parameters_lr, parameters_knn, parameters_rf]\n",
        "        bestModels = []\n",
        "\n",
        "        for i in range(parameters_list.__len__()):\n",
        "            print('Best {} Model-->'.format(model_names[i]))\n",
        "            Grid = GridSearchCV(model_list[i], parameters_list[i], cv = 5).fit(X_train, y_train)\n",
        "            print_grid_search_metrics(Grid, parameters_list[i])\n",
        "            bestModels.append(Grid.best_estimator_)\n",
        "            print('-----------------------------')\n",
        "        def cal_evaluation(classifier, cm):\n",
        "            tn = cm[0][0]\n",
        "            fp = cm[0][1]\n",
        "            fn = cm[1][0]\n",
        "            tp = cm[1][1]\n",
        "            accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
        "            precision = tp / (tp + fp + 0.0)\n",
        "            recall = tp / (tp + fn + 0.0)\n",
        "            print (classifier)\n",
        "            print (\"\\tAccuracy is: %0.3f\" % accuracy)\n",
        "            print (\"\\tprecision is: %0.3f\" % precision)\n",
        "            print (\"\\trecall is: %0.3f\\n\" % recall)\n",
        "\n",
        "        # print out confusion matrices\n",
        "        def draw_confusion_matrices(confusion_matricies):\n",
        "            class_names = ['Bull','Bear']\n",
        "            classifier, cm = confusion_matrices    \n",
        "            print(len(cm), len(cm[0]))     \n",
        "            cal_evaluation(classifier, cm)\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111)\n",
        "            cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n",
        "            plt.title('Confusion matrix for %s' % classifier)\n",
        "            fig.colorbar(cax)\n",
        "            ax.set_xticklabels([''] + class_names)\n",
        "            ax.set_yticklabels([''] + class_names)\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.show()\n",
        "        %matplotlib inline\n",
        "        print(\"=================Confusion Matrix===========================================\")\n",
        "        for i in range(bestModels.__len__()):\n",
        "            confusion_matrices = (model_names[i], confusion_matrix(y_test,bestModels[i].predict(X_test)))\n",
        "            #print(confusion_matrices[0], confusion_matrices[1])\n",
        "            draw_confusion_matrices(confusion_matrices)\n",
        "\n",
        "\"\"\"\n",
        "Model --> LR Regression, KNN, RF\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtnFcZ7CJn9w",
        "colab_type": "text"
      },
      "source": [
        "##3. 对真实数据检验Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMrAd18AJneI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "symbols = ['AAPL','FB','MSFT','NFLX','GILD','MRNA','REGN']\n",
        "for s in symbols:\n",
        "    s = [s]\n",
        "    test = PipeLine(None)\n",
        "    origin = test.GetTiingoData(s)\n",
        "    test.CustomMetrics()\n",
        "    test.TriangleTrend()\n",
        "    test.HeadAndShoulders()\n",
        "    test.GetTargetVar(5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}